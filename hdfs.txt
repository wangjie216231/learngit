这种架构主要由四个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode。下面我们分别介绍这四个组成部分。
1）Client：就是客户端。
        （1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储；
        （2）与NameNode交互，获取文件的位置信息；
        （3）与DataNode交互，读取或者写入数据；
      	（4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS；
        （5）Client可以通过一些命令来访问HDFS；
寻址时间/0.01=传输时间

1.3.1 优点
1）高容错性
        （1）数据自动保存多个副本。它通过增加副本的形式，提高容错性；
        （2）某一个副本丢失以后，它可以自动恢复。
2）适合大数据处理
       （1）数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据；
        （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。
3）流式数据访问，它能保证数据的一致性。
4）可构建在廉价机器上，通过多副本机制，提高可靠性。

1.3.2 缺点
1）不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。
2）无法高效的对大量小文件进行存储。
        （1）存储大量小文件的话，它会占用NameNode大量的内存来存储文件、目录和块信息。这样是不可取的，因为NameNode的内存总是有限的；
        （2）小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。
3）不支持并发写入、文件随机修改。
        （1）一个文件只能有一个写，不允许多个线程同时写；
        （2）仅支持数据append（追加），不支持文件的随机修改。C:\Program Files (x86)\Java\jdk1.6.0_16


环境变量安装：
HADOOP_HOME: hadoop version
MAVEN_HOME:  mvn -v   =====cmd 管理员运行没问题
JAVA_HOME :  java -version


Windows环境下 Hadoop Error: JAVA_HOME is incorrectly set. 问题
	D:\hadoop-2.7.2\etc\hadoop
	改 hadoop-env.cmd 文件 set JAVA_HOME=C:\PROGRA~1\Java\jdk1.8.0_144  或者set JAVA_HOME="C:\Program Files"\Java\jdk1.8.0_77  加引号
	C:\PROGRA~1代表   C:\Program Files

	
C:\Program Files (x86)\Java\jdk1.6.0_16

C:\Program Files (x86)\Java\jdk1.6.0_16、bin
	
	
调教好idea 的配置
1.maven的设置：
	configure->settings
	build->maven
	右边最下三行maven 路径和M2

2.idea工具的基础设置：
（1）搜索auto： 
	自动导包：auto import 选择ask
		
	代码自动补全：code completion 选择first letter（首字母）

	字体大小：
	Editor->font

	搜索reopen：（system settings）:将Reopen last project on startup 前面的对勾去掉，启动idea显示所有工程目录

	搜索file encodings：修改字符编码UTF-8

	注意：以上所有操作都需要在defualt Settings重新设置一遍


4．创建一个Maven工程HdfsClientDemo
5．导入相应的依赖坐标+日志添加
导入D:\hadoop-2.7.2\share\hadoop  ==common  == hdfs
（这里不建议复制粘贴，因为可能是编码不同又或者有中文空格，导致报错，最好是手动敲进去；还有一个就是log4j也是同样的问题，有其他干扰的字符，导致报错）

修改在SRC 下面的pom.xml里面
<project>
<dependencies>
		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<version>RELEASE</version>
		</dependency>
		<dependency>
			<groupId>org.apache.logging.log4j</groupId>
			<artifactId>log4j-core</artifactId>
			<version>2.8.2</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-common</artifactId>
			<version>2.7.2</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-client</artifactId>
			<version>2.7.2</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-hdfs</artifactId>
			<version>2.7.2</version>
		</dependency>
</dependencies>

mvn clean install
</project>

若没有导入依赖包： 进settings -- build.....=--==build tools ===maven -===== repositories ===右边update
项目右键 maven==reimport

IDEA External libraries 不显示Maven中引入的repository：
	1. 打开IDEA右侧的侧边栏Maven Project -> life cycle -> clean
	2. install
	3. 最后同样是在Maven Project 侧边栏中，右键项目名称  -> reimport

注意：如果eclipse/idea打印不出日志，在控制台上只显示
	1.log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).  
	2.log4j:WARN Please initialize the log4j system properly.  
	3.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
	
	需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入
	
	log4j.rootLogger=INFO, stdout
	log4j.appender.stdout=org.apache.log4j.ConsoleAppender
	log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
	log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
	log4j.appender.logfile=org.apache.log4j.FileAppender
	log4j.appender.logfile.File=target/spring.log
	log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
	log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n

	
6．创建包名：com.atguigu.hdfs
7．创建HdfsClient类
	创建@Test的时候  可能没有提示，并且不会自动导包，这个时候添加maven的依赖就可以了
	file -- Project Structure -- Libraries -- + -- java -- intellij的安装目录 -- lib  -- junit : junit-4.12
	
	
	import org.apache.hadoop.conf.Configuration;
	import org.apache.hadoop.fs.FileSystem;
	import org.junit.Test;
	public class HdfsClient{	
	@Test
	public void testMkdirs() throws IOException, InterruptedException, URISyntaxException{
		
		// 1 获取文件系统
		Configuration configuration = new Configuration();//使用org.apache.hadoop.conf
		// 配置在集群上运行
		// configuration.set("fs.defaultFS", "hdfs://hadoop102:9000");
		// FileSystem fs = FileSystem.get(configuration);

		FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");
		
		// 2 创建目录
		fs.mkdirs(new Path("/1108/daxian/banzhang"));
		
		// 3 关闭资源
		fs.close();
	}
}


上传文件====文件在哪
package com.atguigu.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.junit.Test;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

public class HdfsClient {
    @Test
    public void testCopyFromLocalFile() throws IOException, InterruptedException, URISyntaxException {
        // 1 获取文件系统
        Configuration configuration = new Configuration();
        configuration.set("dfs.replication", "2");
        FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");

        // 2 上传文件
        fs.copyFromLocalFile(new Path("d:/eximtrxTabMapping.xml"), new Path("/eximtrxTabMapping.xml"));

        // 3 关闭资源
        fs.close();

        System.out.println("over");
    }
}


将hdfs-site.xml拷贝到项目的根目录下=========根目录在哪？？、
	先包里面的hdfs-default.xml 
	然后根目录里面的hdfs-site.xml
	最后执行代码里面的configuration.set("dfs.replication","2");
	
文件下载
	@Test
	public void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{
		// 1 获取文件系统
		Configuration configuration = new Configuration();
		FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");
		
		// 2 执行下载操作
		// boolean delSrc 指是否将原文件删除
		// Path src 指要下载的文件路径
		// Path dst 指将文件下载到的路径
		// boolean useRawLocalFileSystem 是否开启文件校验
		fs.copyToLocalFile(false, new Path("/hello1.txt"), new Path("e:/hello1.txt"), true);
		//windows文件的斜杠//"D:\\eepar.tar  e:/hello1.txt  都可以
		
		// 3 关闭资源
		fs.close();
}

java.lang.NoClassDefFoundError: org/apache/hadoop/fs/CanUnbuffer
一定是包冲突，观察pom.xml


文件删除：
		fs.delete(new Path("/1108/"), true); 第二个参数是什么



修改文件名称：
		fs.rename(new Path("/hello.txt"), new Path("/hello6.txt"));
		

文件详情：
获取文件详情
		RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), true);
		
		while(listFiles.hasNext()){
			LocatedFileStatus status = listFiles.next();
			
			// 输出详情
			// 文件名称

			// 长度
			System.out.println(status.getLen());
			// 权限
			System.out.println(status.getPermission());
			// z组
			System.out.println(status.getGroup());
			
			// 获取存储的块信息
			BlockLocation[] blockLocations = status.getBlockLocations();
			
			for (BlockLocation blockLocation : blockLocations) {
				
				// 获取块存储的主机节点
				String[] hosts = blockLocation.getHosts();
				
				for (String host : hosts) {
					System.out.println(host);
				}
			}
			
			System.out.println("----------------班长的分割线-----------");
		}		
		
判断是文件还是文件夹判断
		FileStatus[] listStatus = fs.listStatus(new Path("/"));
		
		for (FileStatus fileStatus : listStatus) {
			
			// 如果是文件
			if (fileStatus.isFile()) {
				System.out.println("f:"+fileStatus.getPath().getName());
			}else {
				System.out.println("d:"+fileStatus.getPath().getName());
			}
		}
		
HDFS文件上传
		创建输入流
		FileInputStream fis = new FileInputStream(new File("e:/hello.txt"));

		// 3 获取输出流
		FSDataOutputStream fos = fs.create(new Path("/hello4.txt"));

		// 4 流对拷
		IOUtils.copyBytes(fis, fos, configuration);

		// 5 关闭资源
		IOUtils.closeStream(fis);
		IOUtils.closeStream(fos);

文件下载：
	获取输入流
		FSDataInputStream fis = fs.open(new Path("/hello1.txt"));
		
		// 3 获取输出流
		FileOutputStream fos = new FileOutputStream(new File("e:/hello1.txt"));
		
		// 4 流的对拷
		IOUtils.copyBytes(fis, fos, configuration);
		
		// 5 关闭资源
		IOUtils.closeStream(fis);
		IOUtils.closeStream(fos);
		fs.close();

定位文件读取：分开下载：并合并
	第一块： 获取输入流
		FSDataInputStream fis = fs.open(new Path("/hadoop-2.7.2.tar.gz"));
		
		// 3 创建输出流
		FileOutputStream fos = new FileOutputStream(new File("e:/hadoop-2.7.2.tar.gz.part1"));
		
		// 4 流的拷贝
		byte[] buf = new byte[1024];
		
		for(int i =0 ; i < 1024 * 128; i++){
			fis.read(buf);
			fos.write(buf);
		}
		
		// 5关闭资源
		IOUtils.closeStream(fis);
		IOUtils.closeStream(fos);
		
下载第二块：
		打开输入流
		FSDataInputStream fis = fs.open(new Path("/hadoop-2.7.2.tar.gz"));
		
		// 3 定位输入数据位置
		fis.seek(1024*1024*128);
		
		// 4 创建输出流
		FileOutputStream fos = new FileOutputStream(new File("e:/hadoop-2.7.2.tar.gz.part2"));
		
		// 5 流的对拷
		IOUtils.copyBytes(fis, fos, configuration);
		
		// 6 关闭资源
		IOUtils.closeStream(fis);
		IOUtils.closeStream(fos);
		
合并文件
	在window命令窗口中执行
	type hadoop-2.7.2.tar.gz.part2 >> hadoop-2.7.2.tar.gz.part1
	
	
namenode secondary工作机制
第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。
（2）客户端对元数据进行增删改的请求。
（3）NameNode记录操作日志，更新滚动日志。
（4）NameNode在内存中对数据进行增删改查。
2. 第二阶段：Secondary NameNode工作 
	（1）Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。
	（2）Secondary NameNode请求执行checkpoint。
	（3）NameNode滚动正在写的edits日志。
	（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。
	（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。
	（6）生成新的镜像文件fsimage.chkpoint。
	（7）拷贝fsimage.chkpoint到NameNode。
	（8）NameNode将fsimage.chkpoint重新命名成fsimage。
	
NN和2NN工作机制详解：
	Fsimage：namenode内存中元数据序列化后形成的文件。
	Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。
	namenode启动时，先滚动edits并生成一个空的edits.inprogress，然后加载edits和fsimage到内存中，此时namenode内存就持有最新的元数据信息。
	client开始对namenode发送元数据的增删改查的请求，这些请求的操作首先会被记录的edits.inprogress中（查询元数据的操作不会被记录在edits中，
	因为查询操作不会更改元数据信息），如果此时namenode挂掉，重启后会从edits中读取元数据的信息。然后，namenode会在内存中执行元数据的增删改查的操作。
	由于edits中记录的操作会越来越多，edits文件会越来越大，导致namenode在启动加载edits时会很慢，所以需要对edits和fsimage进行合并（所谓合并，
	就是将edits和fsimage加载到内存中，照着edits中的操作一步步执行，最终形成新的fsimage）。secondarynamenode的作用就是帮助namenode进行edits和fsimage的合并工作。
	secondarynamenode首先会询问namenode是否需要checkpoint（触发checkpoint需要满足两个条件中的任意一个，定时时间到和edits中数据写满了）。
	直接带回namenode是否检查结果。secondarynamenode执行checkpoint操作，首先会让namenode滚动edits并生成一个空的edits.inprogress，
	滚动edits的目的是给edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的edits和fsimage会拷贝到secondarynamenode的本地，
	然后将拷贝的edits和fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给namenode，重命名为fsimage后替换掉原来的fsimage。
	namenode在启动时就只需要加载之前未合并的edits和fsimage即可，因为合并过的edits中的元数据信息已经被记录在fsimage中。
	
	
Fsimage和Edits解析
1. 概念
	namenode被格式化之后，将在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录中产生如下文件
	fsimage_0000000000000000000
	fsimage_0000000000000000000.md5
	seen_txid
	VERSION
	（1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。 
	（2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到edits文件中。 
	（3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字
	（4）每次NameNode启动的时候都会将fsimage文件读入内存，并edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将fsimage和edits文件进行了合并。
思考：如何确定下次开机启动的时候合并哪些edits？


oiv查看fsimage文件
（1）查看oiv和oev命令
	[atguigu@hadoop102 current]$ hdfs
	oiv                  apply the offline fsimage viewer to an fsimage
	oev                 	 apply the offline edits viewer to an edits file
（2）基本语法
	hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径  ==-p 转化的文件类型  -i要转化的文件名称  -o转化后的文件名称
（3）案例实操
	[atguigu@hadoop102 current]$ pwd
	/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current
	[atguigu@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml
	[atguigu@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/fsimage.xml
	将显示的xml文件内容拷贝到eclipse中创建的xml文件中，并格式化。部分显示结果如下。
思考：可以看出，fsimage中没有记录块所对应datanode，为什么？

3. oev查看edits文件
（1）基本语法
	hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径
（2）案例实操
	[atguigu@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml
	[atguigu@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml
	
checkpoint时间设置
（1）通常情况下，SecondaryNameNode每隔一小时执行一次。如果修改在hdfs-site中
	[hdfs-default.xml]
	<property>
	  <name>dfs.namenode.checkpoint.period</name>
	  <value>3600</value>
	</property >
（2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。
	<property>
	  <name>dfs.namenode.checkpoint.txns</name>
	  <value>1000000</value>
	  <description>操作动作次数</description>
	</property>

	<property>
	  <name>dfs.namenode.checkpoint.check.period</name>
	  <value>60</value>
	  <description> 1分钟检查一次操作次数</description>
	</property >
	
	
NameNode故障处理
NameNode故障后，可以采用如下两种方法恢复数据。
	方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；
	1. kill -9 namenode进程
	2. 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）
		[atguigu@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*
	3. 拷贝SecondaryNameNode中数据到原NameNode存储数据目录
		[atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/
	4. 重新启动namenode
	[atguigu@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode
	
	方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。
	1.修改hdfs-site.xml中的
		<property>
		  <name>dfs.namenode.checkpoint.period</name>
		  <value>120</value>
		</property>

		<property>
		  <name>dfs.namenode.name.dir</name>
		  <value>/opt/module/hadoop-2.7.2/data/tmp/dfs/name</value>
		</property>
	2.kill -9 namenode进程
	3.删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）
		[atguigu@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*
	4.如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件
		[atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./

		[atguigu@hadoop102 namesecondary]$ rm -rf in_use.lock

		[atguigu@hadoop102 dfs]$ pwd
		/opt/module/hadoop-2.7.2/data/tmp/dfs

		[atguigu@hadoop102 dfs]$ ls
		data  name  namesecondary
	5.	导入检查点数据（等待一会ctrl+c结束掉）
	[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint
	6.	启动namenode
	[atguigu@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode
	
建文件目录：详解
建文件名称：详解

 集群安全模式
1.	概述
NameNode启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作。一旦在内存中成功建立文件系统元数据的映像，
	则创建一个新的fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。但是此刻，NameNode运行在安全模式，即NameNode的文件系统对于客户端来说是只读的。
	系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。
	在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统。
	如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。
	在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式。
2.基本语法
	集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。
	（1）bin/hdfs dfsadmin -safemode get		（功能描述：查看安全模式状态）
	（2）bin/hdfs dfsadmin -safemode enter  	（功能描述：进入安全模式状态）=====后，不能操作
	（3）bin/hdfs dfsadmin -safemode leave	（功能描述：离开安全模式状态）
	（4）bin/hdfs dfsadmin -safemode wait	（功能描述：等待安全模式状态，监控安全模式）====等待安全模式结束，立即执行操作
				put: Cannot create file/root/hello1.txt._COPYING_. Name node is in safe mode.
	
3.	案例
	模拟等待安全模式
	（1）先进入安全模式
		[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter
	（2）执行下面的脚本
		编辑一个脚本first.sh
		#!/bin/bash
		bin/hdfs dfsadmin -safemode wait
		bin/hdfs dfs -put ~/hello.txt /root/hello.txt

		执行bash first.sh
	（3）再打开一个窗口，执行
		[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave   ======关闭后执行脚本里面的上传操作
		
NameNode多目录配置
1.	NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性
2.	具体配置如下
	（1）在hdfs-site.xml文件中增加如下内容
		<property>
			<name>dfs.namenode.name.dir</name>
		<value>file:///${hadoop.tmp.dir}/dfs/name1,file:///${hadoop.tmp.dir}/dfs/name2</value>
		</property>
		=======
		=====${hadoop.tmp.dir}/即core-site.xml里面hadoop.tmp.dir点的路径/opt/module/hadoop-2.7.2/data/tmp
		
（2）停止集群，删除data和logs中所有数据。
		[atguigu@hadoop102 hadoop-2.7.2]$ rm -rf data/ logs/=====不停止集群无法删除
		[atguigu@hadoop103 hadoop-2.7.2]$ rm -rf data/ logs/
		[atguigu@hadoop104 hadoop-2.7.2]$ rm -rf data/ logs/
（3）格式化集群并启动。
	[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode –format
	[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh
	
	为了生成namenode的fsimage,...name文件夹里面的东西
（4）查看结果
	[atguigu@hadoop102 dfs]$ ll
	总用量 12
	drwx------. 3 atguigu atguigu 4096 12月 11 08:03 data
	drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name1
	drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name2
	
DataNode工作机制
	1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度==不止namenode会存，块数据的校验和==保证数据完整性，以及时间戳。
	2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。
	3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟+30秒没有收到某个DataNode的心跳，则认为该节点不可用。
	4）集群运行中可以安全加入和退出一些机器。
	
数据完整性
	1）当DataNode读取block的时候，它会计算checksum。==校验和？？？？？
	2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。
	3）client读取其他DataNode上的block。
	4）datanode在其文件创建后周期验证checksum，如图3-16所示。
	
掉线时限参数设置
	DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：
	timeout  = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval。==?????在哪可设置hdfs-site.xml 
	而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。
	需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。
	<property>
		<name>dfs.namenode.heartbeat.recheck-interval</name>
		<value>300000</value>
	</property>
	<property>
		<name> dfs.heartbeat.interval </name>
		<value>3</value>
	</property>
	
	
	
服役新数据节点
需求
随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。假如有两套集群，为了避免找不到问题的影响，这一节可以更好的管理节点
1.	环境准备
	（1）克隆一台虚拟机
	（2）修改ip地址和主机名称
	（3）修改xsync文件，增加新增节点的ssh无密登录配置
	（4）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data）
2.	服役新节点具体步骤
	（1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件白名单
名字可以随便写，只要绝对路径指向正确既可以
	[atguigu@hadoop105 hadoop]$ pwd
	/opt/module/hadoop-2.7.2/etc/hadoop
	[atguigu@hadoop105 hadoop]$ touch dfs.hosts
	[atguigu@hadoop105 hadoop]$ vi dfs.hosts
添加如下主机名称（包含新服役的节点）
	hadoop102
	hadoop103
	hadoop104
	hadoop105
	（2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts属性
	<property>
		<name>dfs.hosts</name>
		<value>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts</value>
	</property>
	（3）刷新namenode
		[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes
		Refresh nodes successful
	（4）更新resourcemanager节点
		[atguigu@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes
		17/06/24 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033
		
		50070 页面datanodes 显示admin state列表  dead
		
	（5）在NameNode的slaves文件中增加新主机名称====？？？？？每个节点都要配吗
		增加105  
			hadoop102
			hadoop103
			hadoop104
			hadoop105
	（6）单独命令启动新的数据节点和节点管理器
		[atguigu@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode
		starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-datanode-hadoop105.out

		[atguigu@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager
		starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-atguigu-nodemanager-hadoop105.out
	（7）在web浏览器上检查是否ok
3.	如果数据不均衡，可以用命令实现集群的再平衡===？？？？？怎么知道不均衡 页面block pool used列看百分比
	[atguigu@hadoop102 sbin]$ ./start-balancer.sh
	starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.out
	Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved

	
	
退役旧数据节点
1.在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件
	[atguigu@hadoop102 hadoop]$ pwd
	/opt/module/hadoop-2.7.2/etc/hadoop
	[atguigu@hadoop102 hadoop]$ touch dfs.hosts.exclude
	[atguigu@hadoop102 hadoop]$ vi dfs.hosts.exclude
	添加如下主机名称（要退役的节点）
	hadoop105
2．在namenode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性
	<property>
	<name>dfs.hosts.exclude</name>
		  <value>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude</value>
	</property>
3．刷新namenode、刷新resourcemanager
	[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes
	Refresh nodes successful
	[atguigu@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes
		17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033
4.检查web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点，如图3-17所示

图3-17  退役中
5.等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役，如图3-18所示

图3-18 已退役
[atguigu@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode
stopping datanode
[atguigu@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager
stopping nodemanager
6.	从include文件中删除退役节点，再运行刷新节点的命令
	（1）从namenode的dfs.hosts文件中删除退役节点hadoop105
hadoop102
hadoop103
hadoop104
	（2）刷新namenode，刷新resourcemanager
[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes
Refresh nodes successful
[atguigu@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes
17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033
7.	从namenode的slave文件中删除退役节点hadoop105
hadoop102
hadoop103
hadoop104
8.	如果数据不均衡，可以用命令实现集群的再平衡
[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh 
starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.out
Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved



6.6 Datanode多目录配置=====不需要删除data logs ，需要停止集群吗只需要重启dfs,需要停止集群，不需要关闭yarn,配置需要分发吗？？？？？
1.	datanode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本
2．具体配置如下
hdfs-site.xml
<property>
    <name>dfs.datanode.data.dir</name>
	<value>file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2</value>
</property>

读写文件PPT

No route to host 的时候，有如下几种可能：
	1、对方的域名确实不通
	2、本机自己开了防火墙
	3、本机的etc/hosts 里面没有配置本机的机器名和ip  （可能性最大）
	4、如果配置了dfs.hosts,并hdfs-site.xml添加了，即使加入了黑名单dfs.hosts.exclude，没有删掉hdfs-site.xml,报


第7章 HDFS 2.X新特性
7.1 集群间数据拷贝
1．scp实现两个远程主机之间的文件复制
	scp -r hello.txt root@hadoop103:/user/atguigu/hello.txt		// 推 push
	scp -r root@hadoop103:/user/atguigu/hello.txt  hello.txt		// 拉 pull
	scp -r root@hadoop103:/user/atguigu/hello.txt root@hadoop104:/user/atguigu   //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。

2．采用discp命令实现两个hadoop集群之间的递归数据复制
	[atguigu@hadoop102 hadoop-2.7.2]$  bin/hadoop discp
	hdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt  ====:9000是namenode路径，桥接的，

数据拷贝整理

7.2 Hadoop存档
1．hdfs存储小文件弊端
	寻址时间太长，大于存储时间
	大文件小文件占用namenode内存是一样的
	每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此hadoop存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。
	但注意，存储小文件所需要的磁盘容量和存储这些文件原始内容所需要的磁盘空间相比也不会增多。例如，一个1MB的文件以大小为128MB的块存储，使用的是1MB的磁盘空间，而不是128MB。

2．解决存储小文件办法之一
	Hadoop存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。
	具体说来，Hadoop存档文件对内（对于文件本身）还是一个一个独立文件，对NameNode（对外）而言却是一个整体，减少了NameNode的内存。=====一份har文件多个datanode 一个namenode

3．案例实操
	（1）需要启动yarn进程（存档相当于走的是mapreduce程序）
	[atguigu@hadoop102 hadoop-2.7.2]$ start-yarn.sh
	
	（2）归档文件archive
	把/user/atguigu目录里面的所有文件归档成一个叫myhar.har的归档文件，并把归档后文件存储到/user/my路径下。
	[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName myhar.har -p   =======	//hdfs oiv -p XML  -i dfjklsj -o djflks.xml

	/user/atguigu   /user/my
	
		_SUCCESS
		_index
		_masterindex
		part-0
	
	和mapreduce相关
	
	hdfs dfs
		 fs
		 dfsadmin
		 rmadmin
		 
	hadoop archive  -archiveName -p 
	
	rm -ir
	
	rpm -e --nodeps
   
   （3）查看归档
		[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/my/myhar.har
		[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///user/my/myhar.har
	（4）解归档文件
		[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/my/myhar.har/* /user/atguigu

	
7.3 快照管理
快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。
1．基本语法
	（1）hdfs dfsadmin -allowSnapshot 路径   （功能描述：开启指定目录的快照功能）
	（2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用）
	（3）hdfs dfs -createSnapshot 路径        （功能描述：对目录创建快照）
	（4）hdfs dfs -createSnapshot 路径 名称   （功能描述：指定名称创建快照）
	（5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照）
	（6）hdfs lsSnapshottableDir         	（功能描述：列出当前用户所有可快照目录）
	（7）hdfs snapshotDiff 路径1 路径2 	（功能描述：比较两个快照目录的不同之处）
	（8）hdfs dfs -deleteSnapshot <path> <snapshotName>  （功能描述：删除快照）

2．案例实操
	（1）开启/禁用指定目录的快照功能
		[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/atguigu/data
		[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/atguigu/data
	
	（2）对目录创建快照
		[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/atguigu/data
		通过web访问hdfs://hadoop102:50070/user/atguigu/data/.snapshot/s…..// 快照和源文件使用相同数据
		：http://hadoop102:50070/explorer.html#/user/dataSnapshot/.snapshot
		[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/atguigu/data/.snapshot/
	
	（3）指定名称创建快照
		[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/atguigu/data miao170508
		：将当前的文件目录里面的东西，包括文件文件夹，都建个快照==备份，放到.snapshot隐藏文件夹里面备份名字叫miao170508
	
	（4）重命名快照
		[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/atguigu/data/
		 miao170508 atguigu170508
	
	（5）列出当前用户所有可快照目录
		[atguigu@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir
	
	（6）比较两个快照目录的不同之处  相对于后面的路径来说的  + 表示增 -表示少
		[atguigu@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff /user/atguigu/data/  .        .snapshot/atguigu170508	  ===.表示快照目录 最好左边用最新的目录 ，右边用旧的，新的相对于旧的+ -
															快照目录           当前目录即/user/atguigu/data/
															+ - 多了什么 少了什么   
	（7）恢复快照
		[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp /user/atguigu/input/.snapshot/s20170708-134303.027 /user

7.4 回收站
1．默认回收站，如图3-19所示
	默认值fs.trash.interval=0，0表示禁用回收站，可以设置删除文件的存活时间。
	默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。
	要求fs.trash.checkpoint.interval<=fs.trash.interval。

	图3-19 回收站
	
	新建的trash 看不见，隐藏在用户的/user/root/.Trash,只有删除一次文件过后才能有.Trash文件夹
2．启用回收站
	修改core-site.xml，配置垃圾回收时间为1分钟。到时间就会删除回收站
		<property>
			<name>fs.trash.interval</name>
		<value>1</value>
		</property>
3．查看回收站
	回收站在集群中的；路径：/user/atguigu/.Trash/….
	
4．修改访问垃圾回收站用户名称
	进入垃圾回收站用户名称，默认是dr.who，修改为atguigu用户
	[core-site.xml]
		<property>
		  <name>hadoop.http.staticuser.user</name>
		  <value>atguigu</value>
		</property>
5.	通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站
	Trash trash = New Trash(conf);
	trash.moveToTrash(path);
6.恢复回收站数据
	[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/atguigu/.Trash/Current/user/atguigu/input    /user/atguigu/input
	
	恢复后旧的在回收站里面的就被删除了,mv 失败，如果过了垃圾回收时间会丢失，谨慎使用
	hdfs dfs -mv /user/root/.Trash/190729153200/user/root/hadoop /user/root/hadoop/  ========前面的两个/user/root/hadoop/ 路径必须是对应的，否则失败，后面的
	应该只能恢复原路径
	hdfs dfs -mv /user/root/.Trash/190729160000/user/root/hadoop/ /user  ==user 文件夹里面出现了hadoop文件夹以及里面的文件
	
7.清空回收站
	[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -expunge
	
	
	删除文件夹命令：
	 hdfs dfs -rm -R /user/root/hadoop/hadoop/    =======-rm [-f] [-r|-R] [-skipTrash]  啥意思？？？？？
	
第8章 HDFS HA高可用
8.1 HA概述
	1）所谓HA（high available），即高可用（7*24小时不中断服务）。
	2）实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。
	3）Hadoop2.0之前，在HDFS集群中NameNode存在单点故障（SPOF）。
	4）NameNode主要在以下两个方面影响HDFS集群
		NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启
		NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用
	HDFS HA功能通过配置Active/Standby两个nameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将NameNode很快的切换到另外一台机器。

8.2 HDFS-HA工作机制
	通过双namenode消除单点故障

8.2.1 HDFS-HA工作要点
1.	元数据管理方式需要改变
内存中各自保存一份元数据；
Edits日志只有Active状态的namenode节点可以做写操作；
两个namenode都可以读取edits；
共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现）；
2.	需要一个状态管理功能模块
实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。
3.	必须保证两个NameNode之间能够ssh无密码登录
4.	隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务

8.2.2 HDFS-HA自动故障转移工作机制
前面学习了使用命令hdfs haadmin -failover手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode，下面学习如何配置部署HA自动进行故障转移。自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程，如图3-20所示。ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper的以下功能：
1）故障检测：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。
2）现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。
ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：
1）健康监测：ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。
2）ZooKeeper会话管理：当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。
3）基于ZooKeeper的选择：如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为active状态。

图3-20 HDFS-HA故障转移机制
8.3 HDFS-HA集群配置
8.3.1 环境准备
1.	修改IP
2.	修改主机名及主机名和IP地址的映射
3.	关闭防火墙
4.	ssh免密登录
5.	安装JDK，配置环境变量等
8.3.2 规划集群
表3-1
hadoop102  	hadoop103  	hadoop104
NameNode		NameNode	
JournalNode		JournalNode		JournalNode	
DataNode	DataNode	DataNode
ZK		ZK		ZK	
	ResourceManager	
NodeManager		NodeManager		NodeManager	
	
8.3.3 配置Zookeeper集群
1.	集群规划
在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。
2.	解压安装
（1）解压zookeeper安装包到/opt/module/目录下
[atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/
（2）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData
mkdir -p zkData
（3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg
mv zoo_sample.cfg zoo.cfg
3.	配置zoo.cfg文件
	（1）具体配置
dataDir=/opt/module/zookeeper-3.4.10/zkData
	增加如下配置
#######################cluster##########################
server.2=hadoop102:2888:3888
server.3=hadoop103:2888:3888
server.4=hadoop104:2888:3888
（2）配置参数解读
Server.A=B:C:D。
A是一个数字，表示这个是第几号服务器；
B是这个服务器的ip地址；
C是这个服务器与集群中的Leader服务器交换信息的端口；
D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。
集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。
4.	集群操作
（1）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件
touch myid
添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码
（2）编辑myid文件
	vi myid
	在文件中添加与server对应的编号：如2
（3）拷贝配置好的zookeeper到其他机器上
	scp -r zookeeper-3.4.10/ root@hadoop103.atguigu.com:/opt/app/
	scp -r zookeeper-3.4.10/ root@hadoop104.atguigu.com:/opt/app/
	并分别修改myid文件中内容为3、4
（4）分别启动zookeeper
[root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh start
[root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh start
[root@hadoop104 zookeeper-3.4.10]# bin/zkServer.sh start
（5）查看状态
[root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status
JMX enabled by default
Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg
Mode: follower
[root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status
JMX enabled by default
Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg
Mode: leader
[root@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status
JMX enabled by default
Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg
Mode: follower
8.3.4 配置HDFS-HA集群
1.	官方地址：http://hadoop.apache.org/
2.	在opt目录下创建一个ha文件夹
mkdir ha
3.	将/opt/app/下的 hadoop-2.7.2拷贝到/opt/ha目录下
cp -r hadoop-2.7.2/ /opt/ha/
4.	配置hadoop-env.sh
export JAVA_HOME=/opt/module/jdk1.8.0_144
5.	配置core-site.xml
<configuration>
<!-- 把两个NameNode）的地址组装成一个集群mycluster -->
		<property>
			<name>fs.defaultFS</name>
        	<value>hdfs://mycluster</value>
		</property>

		<!-- 指定hadoop运行时产生文件的存储目录 -->
		<property>
			<name>hadoop.tmp.dir</name>
			<value>/opt/ha/hadoop-2.7.2/data/tmp</value>
		</property>
</configuration>
6.	配置hdfs-site.xml
<configuration>
	<!-- 完全分布式集群名称 -->
	<property>
		<name>dfs.nameservices</name>
		<value>mycluster</value>
	</property>

	<!-- 集群中NameNode节点都有哪些 -->
	<property>
		<name>dfs.ha.namenodes.mycluster</name>
		<value>nn1,nn2</value>
	</property>

	<!-- nn1的RPC通信地址 -->
	<property>
		<name>dfs.namenode.rpc-address.mycluster.nn1</name>
		<value>hadoop102:9000</value>
	</property>

	<!-- nn2的RPC通信地址 -->
	<property>
		<name>dfs.namenode.rpc-address.mycluster.nn2</name>
		<value>hadoop103:9000</value>
	</property>

	<!-- nn1的http通信地址 -->
	<property>
		<name>dfs.namenode.http-address.mycluster.nn1</name>
		<value>hadoop102:50070</value>
	</property>

	<!-- nn2的http通信地址 -->
	<property>
		<name>dfs.namenode.http-address.mycluster.nn2</name>
		<value>hadoop103:50070</value>
	</property>

	<!-- 指定NameNode元数据在JournalNode上的存放位置 -->
	<property>
		<name>dfs.namenode.shared.edits.dir</name>
	<value>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster</value>
	</property>

	<!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->
	<property>
		<name>dfs.ha.fencing.methods</name>
		<value>sshfence</value>
	</property>

	<!-- 使用隔离机制时需要ssh无秘钥登录-->
	<property>
		<name>dfs.ha.fencing.ssh.private-key-files</name>
		<value>/home/atguigu/.ssh/id_rsa</value>
	</property>

	<!-- 声明journalnode服务器存储目录-->
	<property>
		<name>dfs.journalnode.edits.dir</name>
		<value>/opt/ha/hadoop-2.7.2/data/jn</value>
	</property>

	<!-- 关闭权限检查-->
	<property>
		<name>dfs.permissions.enable</name>
		<value>false</value>
	</property>

	<!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式-->
	<property>
  		<name>dfs.client.failover.proxy.provider.mycluster</name>
	<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>
</configuration>
7.	拷贝配置好的hadoop环境到其他节点
8.3.5 启动HDFS-HA集群
1.	在各个JournalNode节点上，输入以下命令启动journalnode服务
	sbin/hadoop-daemon.sh start journalnode
2.	在[nn1]上，对其进行格式化，并启动
	bin/hdfs namenode -format
	sbin/hadoop-daemon.sh start namenode
3.	在[nn2]上，同步nn1的元数据信息
	bin/hdfs namenode -bootstrapStandby
4.	启动[nn2]
	sbin/hadoop-daemon.sh start namenode
5.	查看web页面显示，如图3-21，3-22所示

图3-21  hadoop102(standby)

图3-22  hadoop103(standby)
6.	在[nn1]上，启动所有datanode
	sbin/hadoop-daemons.sh start datanode
7.	将[nn1]切换为Active
	bin/hdfs haadmin -transitionToActive nn1
8.查看是否Active
	bin/hdfs haadmin -getServiceState nn1
8.3.6 配置HDFS-HA自动故障转移
1.	具体配置
	（1）在hdfs-site.xml中增加
<property>
	<name>dfs.ha.automatic-failover.enabled</name>
	<value>true</value>
</property>
	（2）在core-site.xml文件中增加
<property>
	<name>ha.zookeeper.quorum</name>
	<value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>
</property>
2.	启动
	（1）关闭所有HDFS服务：
sbin/stop-dfs.sh
	（2）启动Zookeeper集群：
bin/zkServer.sh start
	（3）初始化HA在Zookeeper中状态：
bin/hdfs zkfc -formatZK
	（4）启动HDFS服务：
sbin/start-dfs.sh
	（5）在各个NameNode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的NameNode就是Active NameNode
sbin/hadoop-daemin.sh start zkfc
3.	验证
	（1）将Active NameNode进程kill
kill -9 namenode的进程id
	（2）将Active NameNode机器断开网络
service network stop
8.4 YARN-HA配置
8.4.1 YARN-HA工作机制
1.	官方文档：
http://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html
2.	YARN-HA工作机制，如图3-23所示

图3-22  YARN-HA工作机制
8.4.2 配置YARN-HA集群
1.	环境准备
（1）修改IP
（2）修改主机名及主机名和IP地址的映射
（3）关闭防火墙
（4）ssh免密登录
（5）安装JDK，配置环境变量等
	（6）配置Zookeeper集群
2.	规划集群
表3-2
hadoop102 	hadoop103  	hadoop104
NameNode		NameNode	
JournalNode		JournalNode		JournalNode	
DataNode	DataNode	DataNode
ZK	ZK	ZK
ResourceManager		ResourceManager		
NodeManager		NodeManager		NodeManager	
3.	具体配置
（1）yarn-site.xml
<configuration>

    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>

    <!--启用resourcemanager ha-->
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
 
    <!--声明两台resourcemanager的地址-->
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>cluster-yarn1</value>
    </property>

    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>

    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>hadoop102</value>
    </property>

    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>hadoop103</value>
    </property>
 
    <!--指定zookeeper集群的地址--> 
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>
    </property>

    <!--启用自动恢复--> 
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>
 
    <!--指定resourcemanager的状态信息存储在zookeeper集群--> 
    <property>
        <name>yarn.resourcemanager.store.class</name>     <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
</property>

</configuration>
	（2）同步更新其他节点的配置信息
4.	启动hdfs 
（1）在各个JournalNode节点上，输入以下命令启动journalnode服务：
sbin/hadoop-daemon.sh start journalnode
（2）在[nn1]上，对其进行格式化，并启动：
bin/hdfs namenode -format
sbin/hadoop-daemon.sh start namenode
（3）在[nn2]上，同步nn1的元数据信息：
bin/hdfs namenode -bootstrapStandby
（4）启动[nn2]：
sbin/hadoop-daemon.sh start namenode
（5）启动所有datanode
sbin/hadoop-daemons.sh start datanode
（6）将[nn1]切换为Active
bin/hdfs haadmin -transitionToActive nn1
5.	启动yarn 
（1）在hadoop102中执行：
sbin/start-yarn.sh
（2）在hadoop103中执行：
sbin/yarn-daemon.sh start resourcemanager
（3）查看服务状态，如图3-24所示
bin/yarn rmadmin -getServiceState rm1

图3-24  YARN的服务状态
8.5 HDFS Federation架构设计
1.	NameNode架构的局限性
（1）Namespace（命名空间）的限制
由于NameNode在内存中存储所有的元数据（metadata），因此单个namenode所能存储的对象（文件+块）数目受到namenode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个datanode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个datanode从4T增长到36T，集群的尺寸增长到8000个datanode。存储的需求从12PB增长到大于100PB。
（2）隔离问题
由于HDFS仅有一个namenode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。
	（3）性能的瓶颈
	由于是单个namenode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个namenode的吞吐量。
2.	HDFS Federation架构设计，如图3-25所示
能不能有多个NameNode
表3-3
NameNode		NameNode		NameNode	
元数据	元数据	元数据
Log		machine	电商数据/话单数据

图3-25  HDFS Federation架构设计
3.	HDFS Federation应用思考
不同应用可以使用不同NameNode进行数据管理
		图片业务、爬虫业务、日志审计业务
Hadoop生态系统中，不同的框架使用不同的namenode进行管理namespace。（隔离性）